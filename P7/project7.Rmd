---
title: "Free Trial Screener A/B test"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(width = 1200)
```
##Experiment overview

At the time of this experiment, Udacity courses currently have two options on the home page: "start free trial", and "access course materials". If the student clicks "start free trial", they will be asked to enter their credit card information, and then they will be enrolled in a free trial for the paid version of the course. After 14 days, they will automatically be charged unless they cancel first. If the student clicks "access course materials", they will be able to view the videos and take the quizzes for free, but they will not receive coaching support or a verified certificate, and they will not submit their final project for feedback.

In the experiment, Udacity tested a change where if the student clicked "start free trial", they were asked how much time they had available to devote to the course. If the student indicated 5 or more hours per week, they would be taken through the checkout process as usual. If they indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free. At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead. [This screenshot](https://goo.gl/haciU6) shows what the experiment looks like.

The hypothesis was that this might set clearer expectations for students upfront, thus reducing the number of frustrated students who left the free trial because they didn't have enough timeâ€”without significantly reducing the number of students to continue past the free trial and eventually complete the course. If this hypothesis held true, Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.

The unit of diversion is a cookie, although if the student enrolls in the free trial, they are tracked by user-id from that point forward. The same user-id cannot enroll in the free trial twice. For users that do not enroll, their user-id is not tracked in the experiment, even if they were signed in when they visited the course overview page.

## Experiment design

In this section I will describe how the experiment is designed.

### Metric choice

Invariant metrics are metrics that are expected to remain (more or less) unchanged between the control and experiment group. Evaluation metrics are the metrics that are expected to change.

* Number of cookies: The number of visitors to the homepage are not expected to change, since the popup is not visible here, so this is an **invariant metric**.
* Number of user-ids: The number of users that enroll in the free trial is expected to drop (because of the popup), so this cannot be an invariant metric. But it is also not a good evaluation metric, because this number depends on the number of clicks which can fluctuate a lot in time. The rate of the number of users between the number of clicks (gross conversion) is a better metric, since it takes into account these fluctuations
* Number of clicks: The number of unique cookies to click the "Start free trial" are not expected to change, since the popup is not yet visible here, so this is an **invariant metric**.
* Click-through-probability: Since both the number of cookies and the number of clicks are not expected to change, this is also an **invariant metric**.
* Gross-conversion: The number of user-ids to enroll in the free trial is expected to drop, while the number of clicks is expected to stay the same, so this rate is expected to drop. This rate can therefore be used as an  **evaluation metric**. The practical significance boundary, which is the boundary in order to assess if the experiment is a success, is 0.01
* Retention: This is an interesting metric because it can be an indication of how motivated users are. If less user-ids enroll, but more of these will remain enrolled, this rate will increase. However, from the analysis which I have done, it follows that it would take too long to gather enough data to evaluate this metric, so we can't use this etric.
* Net conversion: The number of user-ids to remain enrolled past the 14-day boundary is expected to drop, while the number of clicks remains the same, so this rate is also expected to drop and can be used as an **evaluation metric**. The practical significance boundary is 0.0075

### Measuring Standard Deviation

To evaluate whether the analytical estimate of standard deviation is accurate and matches the empirical standard deviation, the unit of analysis and unit of diversion are compared for each evaluation metric. To calculate the standard deviation, a distribution should be known or assumed. A binomial distribution is assumed, since the success probability p can be seen as the case where a user enrolls or clicks and (1-p) the probability the user doesn't. The standard deviation for this distribution is sqrt(p*(1-p)/n), where n is the sample size.

```{r sd, echo=FALSE}
baseline <- read.csv("final_baseline.csv", header = FALSE)
df<-data.frame()
df <- rbind(df, baseline$V2)
colnames(df) <- c("pageviews_pd", "clicks_pd", "enrollments_pd", "ct_prob", "enroll_prob", "payment_enroll_prob", "payment_click_prob")

#calculate standard deviation for binomial distribution, with p the probability of success and n the sample size
calcSD <- function(p,n){
  return(sqrt((p * (1-p) / n)))
}
#sd gross conversion
sample_size <- 5000
gross_n <- sample_size * df$ct_prob
gross_sd <- calcSD(df$enroll_prob, gross_n)
#sd net conversion
net_n <- sample_size * df$ct_prob
net_sd <- calcSD(df$payment_click_prob, net_n)
#put result in df
sd <- as.data.frame(matrix(nrow = 2, ncol = 5))
colnames(sd) <- c("metric", "p", "q=(1-p)", "n", "standard deviation")
sd[1,] <- c("gross conversion", df$enroll_prob, 1-df$enroll_prob, gross_n, gross_sd)
sd[2,] <- c("net conversion", df$payment_click_prob, 1-df$payment_click_prob, net_n, net_sd)
sd
```

When the unit of analysis is equal to the unit of division, the analytical estimate is likely to match the empirical variance. For both metrics, the unit of division and unit of analysis is the cookie. So, the analytical estimate can be used for these metrics.

### Sizing

The Bonfessoni correction is a method used to counteract the problem of incorrectly rejecting the null hypothesis in multiple comparisions. I have decided not to use the Bonfessoni correction, since it is substantially conservative in situations where the tests are highly correlated. The evaluation metrics are highly correlated because they are all direct or indirectly related to the number of clicks  on the "Free trial" button.
I have used the [online calculator](http://www.evanmiller.org/ab-testing/sample-size.html) to retrieve the sample size of the metrics. All calculations have been done with alpha=5% and beta=20%. The pageviews for the metrics can be found below.

```{r, echo=FALSE}
#get sample size with online calculator: http://www.evanmiller.org/ab-testing/sample-size.html
#gross sample size: alpha = 0.05, beta = 0.20, p = 0.20625, dmin = 0.01
gross_sample_size <- 25835
#retention sample size: alpha = 0.05, beta = 0.20, p = 0.53, dmin = 0.01
ret_sample_size <- 39115
#net sample size: alpha = 0.05, beta = 0.20, p = 0.1093125, dmin = 0.0075
net_sample_size <- 27413

#calculate pageviews by click through\enrollment rate and multiply by 2 because sample size is per variation 
gross_pageviews <- gross_sample_size * (1/df$ct_prob) * 2
ret_pageviews <- ret_sample_size * (df$pageviews_pd / df$enrollments_pd) * 2
net_pageviews <- net_sample_size * (1/df$ct_prob) * 2

pageviews <- max(gross_pageviews, net_pageviews)
pageviews_df <- as.data.frame(matrix(nrow = 2, ncol = 7))
colnames(pageviews_df) <- c("metric", "baseline", "dmin", "sample size", "pageviews", "fraction (%)", "days")
pageviews_df[1,] <- c("gross conversion", 0.20625, 0.01, gross_sample_size, gross_pageviews, 100, ceiling(gross_pageviews/ df$pageviews_pd))
pageviews_df[2,] <- c("retention", 0.53, 0.01, ret_sample_size, ret_pageviews, 100, ceiling(ret_pageviews/ df$pageviews_pd))
pageviews_df[3,] <- c("net conversion", 0.1093125, 0.0075, net_sample_size, net_pageviews, 100, ceiling(net_pageviews/ df$pageviews_pd))
pageviews_df
```

###Duration vs. Exposure

The fraction of the traffic diverted to this experiment depends on the risk of the new feature (the 5 hour popup) and how long the experiment is allowed to run.
I think the risk of this experiment is low, since it is only a small feature and the user is only confronted with it in one case (click on "free trial"). For this reason and to make the experiment run not too long, I would divert 100% of the traffice to the experiment. Since, there is a control and experiment group, that both get 50% of the traffic, still 50% will see the website without the new feature. 
Even with 100% divertion, the experiment would take 119 days if we use all metrics. Because this would take too long to evaluate the new feature, retention is skipped as evaluation metric. With gross conversion and net conversion as metrics, the experiment will take 18 days.

## Experiment analysis

###Sanity checks

For each of the invariant metrics, the 95% confidence interval is displayed below. The observed value should be within the confidence interval, so the null hypothesis that the metrics are equal between the control and experiment group is not rejected. As can be seen, for all cases the observed values are within the confidence interval for all metrics, so the sanity checks pass.

```{r, echo=FALSE}
#read exp and control data
control <- read.csv("final_results_control.csv", header = TRUE)
exp <- read.csv("final_results_experiment.csv", header = TRUE)
#calculate Confidence interval based on p, n and z. 
calcCI <- function(p, n, z){
  SD <- calcSD(p, n)  
  ME <- z * SD
  c(p - ME, p + ME)
}
#number of unique cookies to visit overview page=pageviews. p=0.5, n = total amount of clicks, z = 1.96 because of 5% confidence interval (two sided)
total_pageviews_control <- sum(control$Pageviews)
total_pageviews_exp <- sum(exp$Pageviews)
cookies_CI <- calcCI(0.5, (total_pageviews_control + total_pageviews_exp), 1.96)
cookies_observed <- total_pageviews_control / (total_pageviews_control + total_pageviews_exp)

#number of unique cookies to click start free trial=clicks. p=0.5, n = total amount of clicks, z = 1.96 because of 5% confidence interval (two sided)
total_clicks_control <- sum(control$Clicks)
total_clicks_exp <- sum(exp$Clicks)
clicks_CI <- calcCI(0.5, (total_clicks_control + total_clicks_exp), 1.96)
clicks_observed <- total_clicks_control / (total_clicks_control + total_clicks_exp)

#click through rate. Pooled SD is calculated by: sqrt(p * (1-p) / (1/nc + 1/nexp))s
ctr_p_pooled <- (total_clicks_control + total_clicks_exp) / (total_pageviews_control + total_pageviews_exp)
SD <- sqrt(ctr_p_pooled * (1 - ctr_p_pooled) * (1/ total_pageviews_control + 1/ total_pageviews_exp))
ME <- 1.96 * SD
ctr_observed <- (total_clicks_exp / total_pageviews_exp) - (total_clicks_control / total_pageviews_control)

#result df
sanity_df <- as.data.frame(matrix(nrow = 3, ncol = 5))
colnames(sanity_df) <- c("metric", "lower bound", "upper bound", "observed", "passes")
sanity_df[1,] <- c("number of cookies", cookies_CI[1], cookies_CI[2], cookies_observed, "yes")
sanity_df[2,] <- c("number of clicks", clicks_CI[1], clicks_CI[2], clicks_observed, "yes")
sanity_df[3,] <- c("click through rate", -ME, ME, ctr_observed, "yes")
sanity_df
```

###Result analysis

For each evalation metric, I have calcualted the 95% confidence interval around the observed conversion rate. A metric is statistically significant if the value 0 is not in the interval, since in that case it is very unlikely that the difference is caused by chance.
It is practical significant if the interval also doesn't contain the practical significance boundary.
This gross conversion is statistically significant and also practically significant, however the results for the net conversion indicate there is no significant difference.

```{r, echo=FALSE}

#gross conversion. Pooled SD is calculated by: sqrt(p * (1-p) / (1/nc + 1/nexp))
control_without_na <- control[!is.na(control$Enrollments) & !is.na(control$Payments),]
exp_without_na <- exp[!is.na(exp$Enrollments) & !is.na(exp$Payments),]
total_enrollements_control <- sum(control_without_na$Enrollments)
total_enrollements_exp <- sum(exp_without_na$Enrollments)
total_clicks_control <- sum(control_without_na$Clicks)
total_clicks_exp <- sum(exp_without_na$Clicks)
gross_p_pooled <- (total_enrollements_control + total_enrollements_exp) / (total_clicks_control + total_clicks_exp)
gross_SD <- sqrt(gross_p_pooled * (1 - gross_p_pooled) * (1/ total_clicks_control + 1/ total_clicks_exp))
gross_ME <- 1.96 * gross_SD
gross_observed <- (total_enrollements_exp/total_clicks_exp) - (total_enrollements_control/total_clicks_control)

#net conversion. Pooled SD is calculated by: sqrt(p * (1-p) / (1/nc + 1/nexp))
total_payments_control <- sum(control_without_na$Payments)
total_payments_exp <- sum(exp_without_na$Payments)
net_p_pooled <- (total_payments_control + total_payments_exp) / (total_clicks_control + total_clicks_exp)
net_SD <- sqrt(net_p_pooled * (1 - net_p_pooled) * (1/ total_clicks_control + 1/ total_clicks_exp))
net_ME <- 1.96 * net_SD
net_observed <- (total_payments_exp/total_clicks_exp) - (total_payments_control/total_clicks_control)

#result df
effect_df <- as.data.frame(matrix(nrow = 2, ncol = 6))
colnames(effect_df) <- c("metric", "lower", "upper", "dmin", "stat significant", "prac significant")
effect_df[1,] <- c("gross conversion", gross_observed - gross_ME, gross_observed + gross_ME, 0.01, "yes", "yes")
effect_df[2,] <- c("net conversion", net_observed - net_ME, net_observed + net_ME, 0.0075, "no", "no")
effect_df
```


####Sign Tests

Using the day-by-day data, a sign test can be done to check if the result is statistically significant. I have defined a success as when the conversion in the experiment is smaller than in the control since that is what is expected to happen. The p-value can be calculated by using the binomial test, with probability 0.5 and n the number of days measurued. 

```{r, echo=FALSE}
## sign test
number_of_experiments <- 23
gross_conversion_successes <- c(19)
net_conversion_successes <- c(13)
gross_p <- binom.test(gross_conversion_successes, number_of_experiments, p = 0.5, alternative = c("two.sided", "less", "greater"), conf.level = 0.95)
net_p <- binom.test(net_conversion_successes, number_of_experiments, p = 0.5, alternative = c("two.sided", "less", "greater"), conf.level = 0.95)
sign_df <- as.data.frame(matrix(nrow = 2, ncol = 4))
colnames(sign_df) <- c("metric", "p value", "alpha", "statistical significant")
sign_df[1,] <- c("gross conversion", gross_p[[3]], 0.05, "yes")
sign_df[2,] <- c("net conversion", net_p[[3]], 0.05, "no")
sign_df
```

The gross conversion is statistically significant because the p-value is smaller than alpha/2. The p-value for the net conversion is a lot bigger than alpha, so the results for this metric are not statistically significant.

####Summary

Both test indicate the same finding, namely that the metric gross conversion has a significant difference but the net conversion has not. I have decided not to use the Bonferroni correction, since the metrics are highly correlated and the results would probably be too conservative.

###Recommendation

I recommend that the new feature of including the 5 or more hour popup to the start free trial page is not adopted. The A/B test shows that this will not have a practical significant effect on all evaluation metrics.

##Follow-up experiment

A follow-up experiment should have as goal to reduce the number of frustrated students who cancel early in the course. Frustrations can arise from different sources like: the course costs more time than expected or it doesn't lead to the right knowledge/practice to find a job.
I think students will be more motivated if the course is part of a package of courses that will lead to a job offer. For example an introduction to machine learning course is part of the machine learning track and if all classes are completed, the student can be sure that companies are interested in his skills.
In the course overview page, the course will get some indication so the student can see that the course will offer this extra service for him.

The number of pageviews probably won't change, if no time is invested to make the job service program known. So the number of unique ookies to visit the homepage can be used as an invariant metric.
My hypothesis is that the click through rate, the number of unique cookies to click the "Start free trial" divided by the number of unique cookies to visit the homepage will increase. So, the click through rate is the evaluation metric. The unit of diversion is a cookie, since the user doesn't need to be logged in the user-id cannot be used in this case.

